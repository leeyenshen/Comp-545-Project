{"question": "where did they film hot tub time machine", "answer": "don't know. The provided context does not mention where they filmed \"Hot Tub Time Machine.\"", "context": [], "ground_truth": ["Fernie Alpine Resort"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who has the right of way in international waters", "answer": "e provided context does not contain information about the right of way in international waters.", "context": [], "ground_truth": ["Neither vessel"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": false, "lexical_overlap": 0.5, "ground_truth_hallucinated": true}
{"question": "who does annie work for attack on titan", "answer": "don't know. There is no information provided in the given context about Annie working for Attack on Titan.", "context": [], "ground_truth": ["Marley"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when was the immigration reform and control act passed", "answer": "don't know, the provided context does not mention the Immigration Reform and Control Act.", "context": [], "ground_truth": ["November\u00a06, 1986"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when was puerto rico added to the usa", "answer": "don't know. The provided context does not contain information about when Puerto Rico was added to the USA.", "context": [], "ground_truth": ["1950"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who has been chosen for best supporting actress in 64 national filmfare award", "answer": "don't know. The provided context does not contain information about the 64th National Filmfare Awards or the best supporting actress category.", "context": [], "ground_truth": ["Zaira Wasim"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "which side of the white house is the front", "answer": "don't know, the provided context does not contain information about the White House or its location.", "context": [], "ground_truth": ["North"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "names of the metropolitan municipalities in south africa", "answer": "don't know. The provided context does not contain information about the names of metropolitan municipalities in South Africa.", "context": [], "ground_truth": ["Mangaung Metropolitan Municipality", "Nelson Mandela Bay Metropolitan Municipality", "eThekwini Metropolitan Municipality", "City of Tshwane Metropolitan Municipality", "City of Johannesburg Metropolitan Municipality", "Buffalo City Metropolitan Municipality", "City of Ekurhuleni Metropolitan Municipality"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who's hosting the super bowl in 2019", "answer": "don't know. The provided context does not mention the host of the Super Bowl in 2019.", "context": [], "ground_truth": ["Atlanta, Georgia"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "in which year vivo launch its first phone in india", "answer": "don't know, there is no information provided in the context about Vivo launching its first phone in India.", "context": [], "ground_truth": ["2014"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "where does it talk about mary magdalene in the bible", "answer": "don't know. The context provided does not mention Mary Magdalene.", "context": [], "ground_truth": ["New Testament"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who carries the nuclear football for the president", "answer": "don't know. The provided context does not mention who carries the nuclear football for the president.", "context": [], "ground_truth": ["aide-de-camp"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what is the origin of the name cynthia", "answer": "don't know. The provided context does not contain information about the origin of the name Cynthia.", "context": [], "ground_truth": ["Greek"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who is the guy who voiced disney channel", "answer": "don't know. The provided context does not mention the person who voiced the Disney Channel.", "context": [], "ground_truth": ["\"Buzz\" Brainard", "Cameron"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what's the legal marriage age in new york", "answer": "don't know. The provided context does not contain information about the legal marriage age in New York.", "context": [], "ground_truth": ["18"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when do the red hot chili peppers tour", "answer": "don't know. The provided context does not mention the Red Hot Chili Peppers or their tour dates.", "context": [], "ground_truth": ["March 2018"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who plays mavis in the movie hotel transylvania", "answer": "don't know. The provided context does not mention anything about a movie called \"Hotel Transylvania\" or a character named Mavis.", "context": [], "ground_truth": ["Sadie Sandler", "Selena Gomez"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what is the channel number for cartoon network on spectrum", "answer": "e provided context does not contain information about the channel number for Cartoon Network on Spectrum.", "context": [], "ground_truth": ["768 (HD)", "68 (SD)"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": false, "ragas_faithfulness": 1.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when are the fa cup semi finals played", "answer": "don't know. The provided context does not mention the FA Cup semi finals.", "context": [], "ground_truth": ["the new Wembley Stadium"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when did the ipod touch 6 gen came out", "answer": "e iPod Touch 6th generation was announced in mid-2015 and officially released on July 15, 2015.", "context": [], "ground_truth": ["July\u00a015,\u00a02015"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "west ham players in the 1966 world cup", "answer": "don't know. The provided context does not contain information about West Ham players in the 1966 World Cup.", "context": [], "ground_truth": ["Martin Peters", "Geoff Hurst", "Bobby Moore"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who sings the theme song for miami vice", "answer": "don't know.", "context": [], "ground_truth": ["Jan Hammer"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5248289108276367, "lexical_hallucinated": false, "lexical_overlap": 0.5, "ground_truth_hallucinated": true}
{"question": "who sings should have seen it in color", "answer": "don't know. The provided context doesn't contain any information about the song \"Should Have Seen It in Color.\"", "context": [], "ground_truth": ["Jamey Johnson"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who starred in the movie deep water horizon", "answer": "don't know, the provided context doesn't mention anything about the movie Deep Water Horizon.", "context": [], "ground_truth": ["Kurt Russell", "Dylan O'Brien", "John Malkovich", "Kate Hudson", "Gina Rodriguez", "Mark Wahlberg"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when did justin bieber perform at madison square garden", "answer": "don't know. The provided context does not contain information about Justin Bieber performing at Madison Square Garden.", "context": [], "ground_truth": ["August 31, 2010"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "protozoan classification used to be based on their means of", "answer": "don't know. The provided context does not mention anything about protozoan classification.", "context": [], "ground_truth": ["locomotion"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who was the first one to migrate to madinah", "answer": "e provided context does not contain information about the first one to migrate to Medina.", "context": [], "ground_truth": ["Islamic prophet Muhammad"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "ice sheets and tundra are typical of which koppen climate category", "answer": "don't know. The provided context does not contain any information about the Koppen climate category or ice sheets and tundra.", "context": [], "ground_truth": ["polar"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what is the size of the angles of an equilateral triangle", "answer": "e provided context does not contain information about the size of the angles of an equilateral triangle.", "context": [], "ground_truth": ["60\u00b0"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": false, "lexical_overlap": 0.5, "ground_truth_hallucinated": true}
{"question": "when does beaty and the beast take place", "answer": "don't know. The provided context does not mention anything about \"Beauty and the Beast.\"", "context": [], "ground_truth": ["Rococo-era"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who sings done for me with charlie puth", "answer": "don't know.", "context": [], "ground_truth": ["American singer Kehlani"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5248289108276367, "lexical_hallucinated": false, "lexical_overlap": 0.5, "ground_truth_hallucinated": true}
{"question": "what counties are in the 6th congressional district in ga", "answer": "don't know. The provided context does not contain information about the 6th congressional district in GA.", "context": [], "ground_truth": ["Fulton", "DeKalb", "Cobb"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who was heading indian army during bangladesh libration war 1971", "answer": "don't know. The provided documents do not contain any information about the Indian Army during the Bangladesh Liberation War of 1971.", "context": [], "ground_truth": ["Sam Manekshaw"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who wrote the music for somewhere in time", "answer": "e context provided does not contain any information about the music for \"Somewhere in Time.\"", "context": [], "ground_truth": ["John Barry"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who sings the marriage of figaro in shawshank redemption", "answer": "don't know. The context provided does not mention anything about the character who sings \"The Marriage of Figaro\" in Shawshank Redemption.", "context": [], "ground_truth": ["Edith Mathis", "Gundula Janowitz"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who did chris long play football for last year", "answer": "don't know. The provided context does not mention anything about Chris Long playing football for any specific team last year.", "context": [], "ground_truth": ["Eagles"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when did harry potter and the deathly hallows come out", "answer": "don't know. The provided context does not mention anything about Harry Potter and the Deathly Hallows.", "context": [], "ground_truth": ["21 July 2007"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "how many episode of my hero academia season 2", "answer": "don't know. The provided context does not mention the number of episodes in season 2 of My Hero Academia.", "context": [], "ground_truth": ["25"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "during the rebuttal section of a formal debate which team speaks first", "answer": "don't know. The provided context does not contain information about a formal debate or the rebuttal section of one.", "context": [], "ground_truth": ["the affirmative team"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when was the last amendment made to the us constitution", "answer": "don't know, the provided context does not contain information about the last amendment made to the US Constitution.", "context": [], "ground_truth": ["May 5, 1992"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "starch based food to be made into plastic", "answer": "don't know. The provided context does not contain any information about starch based food being made into plastic.", "context": [], "ground_truth": ["corn starch"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when did north korea launch the first missile", "answer": "don't know. The provided context does not contain information about when North Korea launched the first missile.", "context": [], "ground_truth": ["1984"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what mineral in 4 ga sandstones was dated to determine the age of the earth", "answer": "don't know. The provided context does not mention any mineral in 4 ga sandstones that was dated to determine the age of the earth.", "context": [], "ground_truth": ["zircon"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when is the last time a total eclipse happened in the us", "answer": "e provided context does not contain information about the last total eclipse to happen in the US.", "context": [], "ground_truth": ["August 21, 2017"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": false, "lexical_overlap": 0.5, "ground_truth_hallucinated": true}
{"question": "when did india participate in olympics for first time", "answer": "don't know. The provided documents do not mention India's participation in the Olympics.", "context": [], "ground_truth": ["1900"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when is it revealed that luke and leia are siblings", "answer": "don't know. The provided context does not contain any information about Luke and Leia being siblings.", "context": [], "ground_truth": ["In Return of the Jedi"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what is the population of farmington new mexico", "answer": "don't know. The provided context does not contain information about the population of Farmington, New Mexico.", "context": [], "ground_truth": ["41,629"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what was the last year they made the gmc envoy", "answer": "don't know, as the context does not contain any information about the last year the GMC envoy was made.", "context": [], "ground_truth": ["2008"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": false, "lexical_overlap": 0.5, "ground_truth_hallucinated": true}
{"question": "new amsterdam was the main settlement in the dutch colony of", "answer": "w Amsterdam was the main settlement in the Dutch colony of New York.", "context": [], "ground_truth": ["New Netherland"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "in the bible who was thrown into the fiery furnace", "answer": "don't know. The provided context does not contain information about who was thrown into the fiery furnace in the Bible.", "context": [], "ground_truth": ["Meshach", "Abednego", "Shadrach"], "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
