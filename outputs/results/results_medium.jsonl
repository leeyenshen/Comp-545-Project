{"question": "where did they film hot tub time machine", "answer": "don't know, the provided context does not mention where they filmed \"Hot Tub Time Machine.\"", "context": [], "ground_truth": ["Fernie Alpine Resort"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who has the right of way in international waters", "answer": "don't know, the provided context does not contain information about the right of way in international waters.", "context": [], "ground_truth": ["Neither vessel"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": false, "lexical_overlap": 0.5, "ground_truth_hallucinated": true}
{"question": "who does annie work for attack on titan", "answer": "don't know. The provided context does not mention anything about Annie working for Attack on Titan.", "context": [], "ground_truth": ["Marley"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when was the immigration reform and control act passed", "answer": "don't know. The provided context does not mention the Immigration Reform and Control Act.", "context": [], "ground_truth": ["November\u00a06, 1986"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when was puerto rico added to the usa", "answer": "don't know. The provided context does not contain information about when Puerto Rico was added to the USA.", "context": [], "ground_truth": ["1950"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who has been chosen for best supporting actress in 64 national filmfare award", "answer": "don't know, there is no information provided in the context about the winner of the Best Supporting Actress category in 64 National Filmfare Awards.", "context": [], "ground_truth": ["Zaira Wasim"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": false, "ragas_faithfulness": 1.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "which side of the white house is the front", "answer": "don't know. The provided context does not contain information about the White House or its location.", "context": [], "ground_truth": ["North"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "names of the metropolitan municipalities in south africa", "answer": "don't know. The provided context does not mention the names of the metropolitan municipalities in South Africa.", "context": [], "ground_truth": ["Mangaung Metropolitan Municipality", "Nelson Mandela Bay Metropolitan Municipality", "eThekwini Metropolitan Municipality", "City of Tshwane Metropolitan Municipality", "City of Johannesburg Metropolitan Municipality", "Buffalo City Metropolitan Municipality", "City of Ekurhuleni Metropolitan Municipality"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who's hosting the super bowl in 2019", "answer": "don't know. The provided context does not contain information about the host of the Super Bowl in 2019.", "context": [], "ground_truth": ["Atlanta, Georgia"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "in which year vivo launch its first phone in india", "answer": "don't know. The provided context does not contain information about the launch of Vivo's first phone in India.", "context": [], "ground_truth": ["2014"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "where does it talk about mary magdalene in the bible", "answer": "don't know. The provided context does not mention Mary Magdalene.", "context": [], "ground_truth": ["New Testament"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who carries the nuclear football for the president", "answer": "don't know. The context provided doesn't contain information about the nuclear football carried for the president.", "context": [], "ground_truth": ["aide-de-camp"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what is the origin of the name cynthia", "answer": "don't know. The provided context does not contain information about the origin of the name Cynthia.", "context": [], "ground_truth": ["Greek"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who is the guy who voiced disney channel", "answer": "don't know, the provided context does not mention anything about the guy who voiced Disney Channel.", "context": [], "ground_truth": ["\"Buzz\" Brainard", "Cameron"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what's the legal marriage age in new york", "answer": "don't know. The context doesn't provide information about the legal marriage age in New York City.", "context": [], "ground_truth": ["18"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when do the red hot chili peppers tour", "answer": "don't know. The provided context does not contain information about the Red Hot Chili Peppers touring.", "context": [], "ground_truth": ["March 2018"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who plays mavis in the movie hotel transylvania", "answer": "don't know. The provided context does not mention who plays Mavis in the movie \"Hotel Transylvania\".", "context": [], "ground_truth": ["Sadie Sandler", "Selena Gomez"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what is the channel number for cartoon network on spectrum", "answer": "don't know. The provided context does not contain information about the channel number for Cartoon Network on spectrum.", "context": [], "ground_truth": ["768 (HD)", "68 (SD)"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when are the fa cup semi finals played", "answer": "don't know. The provided context does not mention when the FA Cup semi finals are played.", "context": [], "ground_truth": ["the new Wembley Stadium"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when did the ipod touch 6 gen came out", "answer": "e iPod Touch 6th generation was announced and released in mid-2015.", "context": [], "ground_truth": ["July\u00a015,\u00a02015"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "west ham players in the 1966 world cup", "answer": "don't know. The provided context does not contain information about West Ham players in the 1966 FIFA World Cup.", "context": [], "ground_truth": ["Martin Peters", "Geoff Hurst", "Bobby Moore"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who sings the theme song for miami vice", "answer": "don't know.", "context": [], "ground_truth": ["Jan Hammer"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5248289108276367, "lexical_hallucinated": false, "lexical_overlap": 0.5, "ground_truth_hallucinated": true}
{"question": "who sings should have seen it in color", "answer": "don't know. The context does not contain information about who sings \"Should Have Seen It in Color\".", "context": [], "ground_truth": ["Jamey Johnson"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who starred in the movie deep water horizon", "answer": "don't know. The provided context does not mention the movie Deep Water Horizon or its cast.", "context": [], "ground_truth": ["Kurt Russell", "Dylan O'Brien", "John Malkovich", "Kate Hudson", "Gina Rodriguez", "Mark Wahlberg"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": false, "ragas_faithfulness": 1.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when did justin bieber perform at madison square garden", "answer": "don't know. The provided context doesn't contain information about Justin Bieber performing at Madison Square Garden.", "context": [], "ground_truth": ["August 31, 2010"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "protozoan classification used to be based on their means of", "answer": "don't know. The provided context does not contain information about the classification of protozoa.", "context": [], "ground_truth": ["locomotion"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who was the first one to migrate to madinah", "answer": "e provided context does not contain information about who was the first one to migrate to Medina.", "context": [], "ground_truth": ["Islamic prophet Muhammad"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "ice sheets and tundra are typical of which koppen climate category", "answer": "sed on the provided context, ice sheets and tundra are typical of the Koppen climate category Cfa, which represents a humid subtropical climate with hot and humid summers and cold winters with snowfall.", "context": [], "ground_truth": ["polar"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what is the size of the angles of an equilateral triangle", "answer": "don't know. The provided context does not contain information about the size of the angles of an equilateral triangle.", "context": [], "ground_truth": ["60\u00b0"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when does beaty and the beast take place", "answer": "don't know. The provided context does not contain information about the novel \"Beauty and the Beast.\"", "context": [], "ground_truth": ["Rococo-era"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who sings done for me with charlie puth", "answer": "don't know. The context provided doesn't contain any information about the song \"Done for Me\" with Charlie Puth.", "context": [], "ground_truth": ["American singer Kehlani"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what counties are in the 6th congressional district in ga", "answer": "don't know. The provided context does not mention anything about the 6th congressional district in Georgia.", "context": [], "ground_truth": ["Fulton", "DeKalb", "Cobb"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who was heading indian army during bangladesh libration war 1971", "answer": "don't know. The provided context does not contain information about who was heading the Indian Army during the Bangladesh Liberation War of 1971.", "context": [], "ground_truth": ["Sam Manekshaw"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who wrote the music for somewhere in time", "answer": "don't know. The provided context does not mention the music for \"Somewhere in Time\".", "context": [], "ground_truth": ["John Barry"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who sings the marriage of figaro in shawshank redemption", "answer": "don't know. The provided context does not mention anything about the Marriage of Figaro being sung in Shawshank Redemption.", "context": [], "ground_truth": ["Edith Mathis", "Gundula Janowitz"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who did chris long play football for last year", "answer": "don't know, the provided context does not mention who Chris Long played football for last year.", "context": [], "ground_truth": ["Eagles"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when did harry potter and the deathly hallows come out", "answer": "don't know. The provided context does not mention the release date of \"Harry Potter and the Deathly Hallows\".", "context": [], "ground_truth": ["21 July 2007"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "how many episode of my hero academia season 2", "answer": "e provided context does not contain information about the number of episodes in My Hero Academia season 2.", "context": [], "ground_truth": ["25"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "during the rebuttal section of a formal debate which team speaks first", "answer": "don't know. The provided context does not contain information about a formal debate.", "context": [], "ground_truth": ["the affirmative team"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": false, "ragas_faithfulness": 1.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when was the last amendment made to the us constitution", "answer": "don't know. The provided context does not mention the last amendment made to the US Constitution.", "context": [], "ground_truth": ["May 5, 1992"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "starch based food to be made into plastic", "answer": "don't know. The provided context does not contain any information about starch based food being made into plastic.", "context": [], "ground_truth": ["corn starch"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when did north korea launch the first missile", "answer": "don't know. The provided documents do not contain information about the launch of the first missile by North Korea.", "context": [], "ground_truth": ["1984"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what mineral in 4 ga sandstones was dated to determine the age of the earth", "answer": "don't know. The provided context does not mention any mineral in 4 ga sandstones that was dated to determine the age of the earth.", "context": [], "ground_truth": ["zircon"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when is the last time a total eclipse happened in the us", "answer": "don't know, the provided context does not contain information about a total solar eclipse happening in the US.", "context": [], "ground_truth": ["August 21, 2017"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": false, "lexical_overlap": 0.5, "ground_truth_hallucinated": true}
{"question": "when did india participate in olympics for first time", "answer": "don't know. The provided context does discuss Greece's participation in the ancient and modern Olympic Games, as well as India's participation in the modern Olympic Games, but it does not mention the first time India participated in the Olympics.", "context": [], "ground_truth": ["1900"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when is it revealed that luke and leia are siblings", "answer": "don't know. The provided documents do not contain any information about the reveal of Luke and Leia being siblings.", "context": [], "ground_truth": ["In Return of the Jedi"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what is the population of farmington new mexico", "answer": "don't know. The provided context does not contain information about the population of Farmington, New Mexico.", "context": [], "ground_truth": ["41,629"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what was the last year they made the gmc envoy", "answer": "don't have access to the information you're asking for. Can you please provide more context or a specific source for this information?", "context": [], "ground_truth": ["2008"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "new amsterdam was the main settlement in the dutch colony of", "answer": "w Amsterdam was the main settlement in the Dutch colony of New York.", "context": [], "ground_truth": ["New Netherland"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "in the bible who was thrown into the fiery furnace", "answer": "don't know. The provided context does not mention anything about the Bible or a person being thrown into a fiery furnace.", "context": [], "ground_truth": ["Meshach", "Abednego", "Shadrach"], "quality_tier": "medium", "num_relevant": 2, "num_distractors": 3, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
