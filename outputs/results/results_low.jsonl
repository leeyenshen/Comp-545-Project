{"question": "where did they film hot tub time machine", "ground_truth": ["Fernie Alpine Resort"], "answer": "don't know. The provided context does not mention where they filmed \"Hot Tub Time Machine.\"", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.25, "ground_truth_hallucinated": true}
{"question": "who has the right of way in international waters", "ground_truth": ["Neither vessel"], "answer": "e provided context does not contain information about the right of way in international waters.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.5555555555555556, "ground_truth_hallucinated": true}
{"question": "who does annie work for attack on titan", "ground_truth": ["Marley"], "answer": "don't know. There is no information provided in the given context about Annie working for Attack on Titan.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when was the immigration reform and control act passed", "ground_truth": ["November\u00a06, 1986"], "answer": "don't know, the provided context does not mention the Immigration Reform and Control Act.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.05555555555555555, "ground_truth_hallucinated": true}
{"question": "when was puerto rico added to the usa", "ground_truth": ["1950"], "answer": "don't know. The provided context does not contain information about when Puerto Rico was added to the USA.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.4, "ground_truth_hallucinated": true}
{"question": "who has been chosen for best supporting actress in 64 national filmfare award", "ground_truth": ["Zaira Wasim"], "answer": "don't know. The provided context does not contain information about the 64th National Filmfare Awards or the best supporting actress category.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.35714285714285715, "ground_truth_hallucinated": true}
{"question": "which side of the white house is the front", "ground_truth": ["North"], "answer": "don't know, the provided context does not contain information about the White House or its location.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.05555555555555555, "ground_truth_hallucinated": true}
{"question": "names of the metropolitan municipalities in south africa", "ground_truth": ["Mangaung Metropolitan Municipality", "Nelson Mandela Bay Metropolitan Municipality", "eThekwini Metropolitan Municipality", "City of Tshwane Metropolitan Municipality", "City of Johannesburg Metropolitan Municipality", "Buffalo City Metropolitan Municipality", "City of Ekurhuleni Metropolitan Municipality"], "answer": "don't know. The provided context does not contain information about the names of metropolitan municipalities in South Africa.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.5909090909090909, "ground_truth_hallucinated": true}
{"question": "who's hosting the super bowl in 2019", "ground_truth": ["Atlanta, Georgia"], "answer": "don't know. The provided context does not mention the host of the Super Bowl in 2019.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.6111111111111112, "ground_truth_hallucinated": true}
{"question": "in which year vivo launch its first phone in india", "ground_truth": ["2014"], "answer": "don't know, there is no information provided in the context about Vivo launching its first phone in India.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.05, "ground_truth_hallucinated": true}
{"question": "where does it talk about mary magdalene in the bible", "ground_truth": ["New Testament"], "answer": "don't know. The context provided does not mention Mary Magdalene.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.3214285714285714, "ground_truth_hallucinated": true}
{"question": "who carries the nuclear football for the president", "ground_truth": ["aide-de-camp"], "answer": "don't know. The provided context does not mention who carries the nuclear football for the president.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.5555555555555556, "ground_truth_hallucinated": true}
{"question": "what is the origin of the name cynthia", "ground_truth": ["Greek"], "answer": "don't know. The provided context does not contain information about the origin of the name Cynthia.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.05555555555555555, "ground_truth_hallucinated": true}
{"question": "who is the guy who voiced disney channel", "ground_truth": ["\"Buzz\" Brainard", "Cameron"], "answer": "don't know. The provided context does not mention the person who voiced the Disney Channel.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.3055555555555556, "ground_truth_hallucinated": true}
{"question": "what's the legal marriage age in new york", "ground_truth": ["18"], "answer": "don't know. The provided context does not contain information about the legal marriage age in New York.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.5909090909090909, "ground_truth_hallucinated": true}
{"question": "when do the red hot chili peppers tour", "ground_truth": ["March 2018"], "answer": "don't know. The provided context does not mention the Red Hot Chili Peppers or their tour dates.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.38636363636363635, "ground_truth_hallucinated": true}
{"question": "who plays mavis in the movie hotel transylvania", "ground_truth": ["Sadie Sandler", "Selena Gomez"], "answer": "don't know. The provided context does not mention anything about a movie called \"Hotel Transylvania\" or a character named Mavis.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.24358974358974358, "ground_truth_hallucinated": true}
{"question": "what is the channel number for cartoon network on spectrum", "ground_truth": ["768 (HD)", "68 (SD)"], "answer": "e provided context does not contain information about the channel number for Cartoon Network on Spectrum.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.05, "ground_truth_hallucinated": true}
{"question": "when are the fa cup semi finals played", "ground_truth": ["the new Wembley Stadium"], "answer": "don't know. The provided context does not mention the FA Cup semi finals.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.6666666666666666, "ground_truth_hallucinated": true}
{"question": "when did the ipod touch 6 gen came out", "ground_truth": ["July\u00a015,\u00a02015"], "answer": "e iPod Touch 6th generation was announced in mid-2015 and officially released on July 15, 2015.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.9166666666666667, "ground_truth_hallucinated": true}
{"question": "west ham players in the 1966 world cup", "ground_truth": ["Martin Peters", "Geoff Hurst", "Bobby Moore"], "answer": "don't know. The provided context does not contain information about West Ham players in the 1966 World Cup.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.4583333333333333, "ground_truth_hallucinated": true}
{"question": "who sings the theme song for miami vice", "ground_truth": ["Jan Hammer"], "answer": "don't know.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.5, "ground_truth_hallucinated": true}
{"question": "who sings should have seen it in color", "ground_truth": ["Jamey Johnson"], "answer": "don't know. The provided context doesn't contain any information about the song \"Should Have Seen It in Color.\"", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.26666666666666666, "ground_truth_hallucinated": true}
{"question": "who starred in the movie deep water horizon", "ground_truth": ["Kurt Russell", "Dylan O'Brien", "John Malkovich", "Kate Hudson", "Gina Rodriguez", "Mark Wahlberg"], "answer": "don't know, the provided context doesn't mention anything about the movie Deep Water Horizon.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when did justin bieber perform at madison square garden", "ground_truth": ["August 31, 2010"], "answer": "don't know. The provided context does not contain information about Justin Bieber performing at Madison Square Garden.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.16666666666666666, "ground_truth_hallucinated": true}
{"question": "protozoan classification used to be based on their means of", "ground_truth": ["locomotion"], "answer": "don't know. The provided context does not mention anything about protozoan classification.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.5, "ground_truth_hallucinated": true}
{"question": "who was the first one to migrate to madinah", "ground_truth": ["Islamic prophet Muhammad"], "answer": "e provided context does not contain information about the first one to migrate to Medina.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.6111111111111112, "ground_truth_hallucinated": true}
{"question": "ice sheets and tundra are typical of which koppen climate category", "ground_truth": ["polar"], "answer": "don't know. The provided context does not contain any information about the Koppen climate category or ice sheets and tundra.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.2916666666666667, "ground_truth_hallucinated": true}
{"question": "what is the size of the angles of an equilateral triangle", "ground_truth": ["60\u00b0"], "answer": "e provided context does not contain information about the size of the angles of an equilateral triangle.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.5, "ground_truth_hallucinated": true}
{"question": "when does beaty and the beast take place", "ground_truth": ["Rococo-era"], "answer": "don't know. The provided context does not mention anything about \"Beauty and the Beast.\"", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.16666666666666666, "ground_truth_hallucinated": true}
{"question": "who sings done for me with charlie puth", "ground_truth": ["American singer Kehlani"], "answer": "don't know.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.5, "ground_truth_hallucinated": true}
{"question": "what counties are in the 6th congressional district in ga", "ground_truth": ["Fulton", "DeKalb", "Cobb"], "answer": "don't know. The provided context does not contain information about the 6th congressional district in GA.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.5, "ground_truth_hallucinated": true}
{"question": "who was heading indian army during bangladesh libration war 1971", "ground_truth": ["Sam Manekshaw"], "answer": "don't know. The provided documents do not contain any information about the Indian Army during the Bangladesh Liberation War of 1971.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.29166666666666663, "ground_truth_hallucinated": true}
{"question": "who wrote the music for somewhere in time", "ground_truth": ["John Barry"], "answer": "e context provided does not contain any information about the music for \"Somewhere in Time.\"", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.4375, "ground_truth_hallucinated": true}
{"question": "who sings the marriage of figaro in shawshank redemption", "ground_truth": ["Edith Mathis", "Gundula Janowitz"], "answer": "don't know. The context provided does not mention anything about the character who sings \"The Marriage of Figaro\" in Shawshank Redemption.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.16666666666666666, "ground_truth_hallucinated": true}
{"question": "who did chris long play football for last year", "ground_truth": ["Eagles"], "answer": "don't know. The provided context does not mention anything about Chris Long playing football for any specific team last year.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.35714285714285715, "ground_truth_hallucinated": true}
{"question": "when did harry potter and the deathly hallows come out", "ground_truth": ["21 July 2007"], "answer": "don't know. The provided context does not mention anything about Harry Potter and the Deathly Hallows.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.16666666666666666, "ground_truth_hallucinated": true}
{"question": "how many episode of my hero academia season 2", "ground_truth": ["25"], "answer": "don't know. The provided context does not mention the number of episodes in season 2 of My Hero Academia.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.38636363636363635, "ground_truth_hallucinated": true}
{"question": "during the rebuttal section of a formal debate which team speaks first", "ground_truth": ["the affirmative team"], "answer": "don't know. The provided context does not contain information about a formal debate or the rebuttal section of one.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.5454545454545454, "ground_truth_hallucinated": true}
{"question": "when was the last amendment made to the us constitution", "ground_truth": ["May 5, 1992"], "answer": "don't know, the provided context does not contain information about the last amendment made to the US Constitution.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.09090909090909091, "ground_truth_hallucinated": true}
{"question": "starch based food to be made into plastic", "ground_truth": ["corn starch"], "answer": "don't know. The provided context does not contain any information about starch based food being made into plastic.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.5909090909090909, "ground_truth_hallucinated": true}
{"question": "when did north korea launch the first missile", "ground_truth": ["1984"], "answer": "don't know. The provided context does not contain information about when North Korea launched the first missile.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.38636363636363635, "ground_truth_hallucinated": true}
{"question": "what mineral in 4 ga sandstones was dated to determine the age of the earth", "ground_truth": ["zircon"], "answer": "don't know. The provided context does not mention any mineral in 4 ga sandstones that was dated to determine the age of the earth.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.5384615384615384, "ground_truth_hallucinated": true}
{"question": "when is the last time a total eclipse happened in the us", "ground_truth": ["August 21, 2017"], "answer": "e provided context does not contain information about the last total eclipse to happen in the US.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.6, "ground_truth_hallucinated": true}
{"question": "when did india participate in olympics for first time", "ground_truth": ["1900"], "answer": "don't know. The provided documents do not mention India's participation in the Olympics.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.16666666666666666, "ground_truth_hallucinated": true}
{"question": "when is it revealed that luke and leia are siblings", "ground_truth": ["In Return of the Jedi"], "answer": "don't know. The provided context does not contain any information about Luke and Leia being siblings.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.4444444444444444, "ground_truth_hallucinated": true}
{"question": "what is the population of farmington new mexico", "ground_truth": ["41,629"], "answer": "don't know. The provided context does not contain information about the population of Farmington, New Mexico.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.26666666666666666, "ground_truth_hallucinated": true}
{"question": "what was the last year they made the gmc envoy", "ground_truth": ["2008"], "answer": "don't know, as the context does not contain any information about the last year the GMC envoy was made.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.6, "ground_truth_hallucinated": true}
{"question": "new amsterdam was the main settlement in the dutch colony of", "ground_truth": ["New Netherland"], "answer": "w Amsterdam was the main settlement in the Dutch colony of New York.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": false, "lexical_overlap": 0.5833333333333333, "ground_truth_hallucinated": true}
{"question": "in the bible who was thrown into the fiery furnace", "ground_truth": ["Meshach", "Abednego", "Shadrach"], "answer": "don't know. The provided context does not contain information about who was thrown into the fiery furnace in the Bible.", "quality_tier": "low", "num_relevant": 1, "num_distractors": 4, "ragas_hallucinated": null, "ragas_faithfulness": null, "nli_hallucinated": null, "nli_entailment_prob": null, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
