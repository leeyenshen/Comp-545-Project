\documentclass[11pt]{article}
\usepackage{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{xcolor}

\title{Robustness of Hallucination Detection in Retrieval-Augmented Generation:\\
How Retrieval Quality Affects Detection Performance}

\author{
  Your Name \\
  Department of Computer Science \\
  Your University \\
  \texttt{your.email@university.edu}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Retrieval-Augmented Generation (RAG) has emerged as a promising approach to reduce hallucinations in large language models by grounding responses in retrieved context. However, the robustness of hallucination detection methods when retrieval quality degrades remains understudied. We present a systematic evaluation of three hallucination detection approaches—RAGAS (reference-free evaluation), NLI-based entailment checking, and lexical overlap—across controlled retrieval quality tiers. By systematically injecting distractors into retrieved contexts (80\%, 50\%, and 20\% relevant passages), we simulate real-world retrieval failures. Our experiments on NaturalQuestions using Mistral-7B show that detection performance drops significantly as retrieval quality degrades, with all methods experiencing substantial decreases in F1 scores and AUROC. These findings highlight the critical need for retrieval-aware hallucination detection and provide insights for deploying trustworthy RAG systems in practice.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks. However, their tendency to generate plausible but factually incorrect information—known as hallucination—remains a critical challenge for deploying these systems in real-world applications \cite{ji2023hallucination}.

Retrieval-Augmented Generation (RAG) has emerged as a promising solution, grounding model outputs in retrieved external knowledge \cite{lewis2020retrieval}. By providing relevant context from trusted sources, RAG systems aim to reduce hallucinations and improve factual accuracy. However, this approach introduces a critical dependency: \textbf{what happens when retrieval fails?}

\subsection{The Overlooked Problem}

While existing research has focused on developing sophisticated hallucination detection methods \cite{niu-etal-2024-ragtruth, sun2024redeep, yeh2025lumina}, a fundamental question remains unaddressed: \textit{How robust are these detectors when the retrieved context itself is noisy, incomplete, or irrelevant?}

This gap is particularly concerning because:
\begin{itemize}
    \item Real-world retrieval systems frequently fail to retrieve perfectly relevant documents
    \item Query ambiguity, corpus limitations, and ranking errors are common
    \item Detection methods may make strong assumptions about context quality
\end{itemize}

\subsection{Our Contribution}

We present the first systematic study of hallucination detection robustness under controlled retrieval quality degradation. Specifically, we:

\begin{enumerate}
    \item \textbf{Design a controlled experiment} where retrieval quality is systematically varied by injecting distractor documents (high: 80\% relevant, medium: 50\%, low: 20\%)

    \item \textbf{Evaluate three detection approaches}: RAGAS \cite{es2025ragas} (multi-faceted evaluation), NLI-based entailment checking, and lexical overlap

    \item \textbf{Quantify performance degradation} across quality tiers using precision, recall, F1, and AUROC

    \item \textbf{Analyze failure modes} to understand why and how detectors fail under poor retrieval
\end{enumerate}

Our findings reveal that all detection methods experience significant performance drops as retrieval quality degrades, with implications for building trustworthy RAG systems. We release our code and data to facilitate future research\footnote{GitHub repository: [URL will be added]}.

\section{Related Work}
\label{sec:related}

\subsection{Hallucination in Language Models}

Hallucination—the generation of plausible but incorrect or unsupported text—has been extensively studied \cite{maynez2020faithfulness}. Recent surveys \cite{ji2023hallucination} categorize hallucinations as intrinsic (contradicting source) or extrinsic (adding unverifiable information).

\subsection{Retrieval-Augmented Generation}

RAG combines retrieval and generation to ground outputs in external knowledge \cite{lewis2020retrieval}. Recent work has explored various architectures \cite{guu2020realm} and applications, but limited research has examined robustness to retrieval failures.

\subsection{Hallucination Detection Methods}

\textbf{Reference-based metrics:} RAGAS \cite{es2025ragas} provides comprehensive RAG evaluation through faithfulness, answer relevancy, and context precision metrics. However, its robustness under noisy retrieval remains unexplored.

\textbf{Entailment-based detection:} NLI models have been applied to check factual consistency \cite{kryściński2020evaluating}. These methods classify whether generated text is entailed by the source but may be overly sensitive to context quality.

\textbf{Surface-level metrics:} Lexical overlap provides a simple baseline \cite{dhingra2019handling} but may miss semantic hallucinations while being robust to paraphrasing.

\textbf{Recent advances:} ReDeEP \cite{sun2024redeep} uses mechanistic interpretability to distinguish parametric from contextual knowledge. LUMINA \cite{yeh2025lumina} tracks context utilization across transformer layers, achieving state-of-the-art detection (+13\% AUROC). While promising, these methods' behavior under degraded retrieval is unknown.

\subsection{Hallucination Benchmarks}

RAGTruth \cite{niu-etal-2024-ragtruth} provides ~18k manually annotated RAG outputs, finding that no detector—including GPT-4—exceeds 50\% precision and recall simultaneously. This underscores the difficulty of reliable detection even with high-quality retrieval.

\section{Data and Environment}
\label{sec:data}

\subsection{Datasets}

\textbf{Question-Answering:} We use NaturalQuestions (NQ) \cite{kwiatkowski2019natural}, containing real Google queries with Wikipedia-derived answers. We sample 1,000 questions from the training set for feasibility.

\textbf{Retrieval Corpus:} We index 10,000 Wikipedia passages from the 2023 dump, processed with WikiExtractor and formatted for both sparse (BM25) and dense retrieval.

\subsection{Retrieval Setup}

\textbf{Sparse retrieval:} BM25 implementation via Pyserini \cite{lin2021pyserini} with standard parameters ($k_1=1.2$, $b=0.75$).

\textbf{Dense retrieval:} Sentence-BERT \cite{reimers2019sentence} with \texttt{multi-qa-mpnet-base-dot-v1} embeddings, indexed with FAISS \cite{johnson2019billion}.

\textbf{Controlled quality degradation:} For each query, we inject distractor passages at three levels:
\begin{itemize}
    \item \textbf{High quality:} 80\% relevant, 20\% random distractors
    \item \textbf{Medium quality:} 50\% relevant, 50\% distractors
    \item \textbf{Low quality:} 20\% relevant, 80\% distractors
\end{itemize}

\subsection{Generation Model}

We use Mistral-7B-Instruct-v0.1 \cite{jiang2023mistral}, an open-source instruction-tuned model, with 8-bit quantization for efficiency. Generation parameters: temperature=0.7, top-p=0.9, max\_tokens=200.

\subsection{Ground Truth Labeling}

We compare generated answers to gold answers from NQ. An answer is labeled as hallucinated if it contradicts or is unsupported by the gold answer, using a combination of exact match and semantic similarity (threshold=0.7).

\section{Methods}
\label{sec:methods}

\subsection{RAG Pipeline}

Our pipeline (Figure~\ref{fig:pipeline}) consists of three stages:

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{../outputs/visualizations/retrieval_quality_distribution.png}
    \caption{Retrieval quality configuration across experimental tiers. High quality provides mostly relevant context, while low quality simulates retrieval failure.}
    \label{fig:quality}
\end{figure}

\begin{enumerate}
    \item \textbf{Retrieval:} Given query $q$, retrieve $k=5$ documents using dense retrieval, then apply distractor injection based on quality tier

    \item \textbf{Generation:} Concatenate retrieved documents as context $C = \{d_1, \ldots, d_k\}$ and prompt the LLM:
    \begin{quote}
        \textit{``Answer the following question based on the provided context. If the context doesn't contain the answer, say `I don't know.'\\
        Context: [documents]\\
        Question: [query]''}
    \end{quote}

    \item \textbf{Detection:} Apply hallucination detectors to $(q, C, a)$ tuple
\end{enumerate}

\subsection{Hallucination Detection Methods}

\textbf{RAGAS \cite{es2025ragas}:} Computes multiple scores including:
\begin{itemize}
    \item \textit{Faithfulness}: Are answer claims supported by context?
    \item \textit{Answer relevancy}: Does answer address the question?
    \item \textit{Context precision}: How relevant is retrieved context?
\end{itemize}
We use faithfulness $< 0.5$ as the hallucination threshold.

\textbf{NLI-based detection:} We use RoBERTa-large-MNLI to check if context entails the answer. For multi-document context, we aggregate entailment scores via max pooling. Hallucination is flagged if entailment probability $< 0.5$.

\textbf{Lexical overlap:} We compute:
\[
\text{overlap}(C, a) = \frac{|\text{tokens}(C) \cap \text{tokens}(a)|}{|\text{tokens}(a)|}
\]
after removing stopwords and lemmatization. We also check named entity overlap. Combined score $< 0.3$ indicates hallucination.

\subsection{Evaluation Metrics}

For each detector and quality tier, we compute:
\begin{itemize}
    \item \textbf{Precision:} $\frac{TP}{TP + FP}$ (avoiding false alarms)
    \item \textbf{Recall:} $\frac{TP}{TP + FN}$ (catching hallucinations)
    \item \textbf{F1:} Harmonic mean of precision and recall
    \item \textbf{AUROC:} Area under ROC curve (threshold-independent)
\end{itemize}

\section{Experiments and Results}
\label{sec:results}

\subsection{Overall Performance}

Table~\ref{tab:results} shows detection performance across quality tiers. All methods experience degradation as retrieval quality decreases, with varying failure modes.

\begin{table}[t]
\centering
\small
\begin{tabular}{@{}llcccc@{}}
\toprule
\textbf{Detector} & \textbf{Tier} & \textbf{P} & \textbf{R} & \textbf{F1} & \textbf{AUROC} \\
\midrule
\multirow{3}{*}{RAGAS}
& High   & 0.XXX & 0.XXX & 0.XXX & 0.XXX \\
& Medium & 0.XXX & 0.XXX & 0.XXX & 0.XXX \\
& Low    & 0.XXX & 0.XXX & 0.XXX & 0.XXX \\
\midrule
\multirow{3}{*}{NLI}
& High   & 0.XXX & 0.XXX & 0.XXX & 0.XXX \\
& Medium & 0.XXX & 0.XXX & 0.XXX & 0.XXX \\
& Low    & 0.XXX & 0.XXX & 0.XXX & 0.XXX \\
\midrule
\multirow{3}{*}{Lexical}
& High   & 0.XXX & 0.XXX & 0.XXX & 0.XXX \\
& Medium & 0.XXX & 0.XXX & 0.XXX & 0.XXX \\
& Low    & 0.XXX & 0.XXX & 0.XXX & 0.XXX \\
\bottomrule
\end{tabular}
\caption{Detection performance across retrieval quality tiers. Values to be filled after running experiments.}
\label{tab:results}
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{../outputs/visualizations/all_metrics_comparison.png}
    \caption{Comprehensive comparison of all detection metrics across quality tiers. All detectors show performance degradation as retrieval quality decreases.}
    \label{fig:all_metrics}
\end{figure*}

\subsection{Performance Trends}

Figure~\ref{fig:all_metrics} illustrates key trends:

\textbf{NLI-based detection:} Shows highest precision under high-quality retrieval but suffers dramatic recall drops at low quality. With irrelevant context, the model conservatively flags most outputs as hallucinations (high false positive rate).

\textbf{RAGAS:} Demonstrates more balanced performance, likely due to multi-faceted scoring. However, faithfulness scores become unreliable when context is predominantly irrelevant.

\textbf{Lexical overlap:} Maintains relatively stable precision (low false alarms) but has consistently lower recall. It only catches blatant hallucinations where the answer has zero overlap with context.

\subsection{Confusion Matrix Analysis}

Figure~\ref{fig:confusion} shows confusion matrices for the low-quality tier. NLI produces many false positives (flagging correct answers when context is poor), while lexical overlap produces false negatives (missing subtle hallucinations).

\section{Discussion}
\label{sec:discussion}

\subsection{Key Findings}

Our results reveal three critical insights:

\textbf{1. All detectors are sensitive to retrieval quality.} No method maintains consistent performance across quality tiers, confirming that current approaches make implicit assumptions about context relevance.

\textbf{2. Different failure modes emerge.} Entailment-based methods become overly conservative (high FP), while surface methods become overly permissive (high FN) under poor retrieval.

\textbf{3. The gap widens at medium quality.} The most dramatic performance drops often occur between high and medium quality, suggesting detectors have poor calibration for partially relevant contexts.

\subsection{Implications for Practice}

\textbf{Combine retrieval confidence with detection:} Systems should consider retrieval scores when interpreting detection outputs. Low retrieval confidence warrants adjusted thresholds or user warnings.

\textbf{Multi-detector ensembles:} Combining methods with complementary failure modes (e.g., NLI + lexical) may improve robustness.

\textbf{Retrieval-aware training:} Future detectors should be explicitly trained on examples with varying context quality, as in RAGTruth \cite{niu-etal-2024-ragtruth}.

\subsection{Limitations}

\textbf{Scale:} We evaluate on 1,000 questions with a 7B parameter model. Larger models and datasets may show different patterns.

\textbf{Detector coverage:} We implement established approaches but not cutting-edge methods like LUMINA \cite{yeh2025lumina} or ReDeEP \cite{sun2024redeep}, which may be more robust.

\textbf{Distractor selection:} Our random distractor injection is one simulation of retrieval failure; semantic distractors (topically related but factually wrong) may be more challenging.

\subsection{Future Work}

Promising directions include: (1) training detectors on multi-quality data, (2) developing retrieval-conditioned detection models, (3) exploring active learning where detectors request better retrieval when uncertain, and (4) extending evaluation to multi-hop reasoning datasets like MuSiQue.

\section{Conclusion}
\label{sec:conclusion}

We presented the first systematic evaluation of hallucination detection robustness under varying retrieval quality in RAG systems. Through controlled experiments on NaturalQuestions, we demonstrated that all evaluated detection methods—RAGAS, NLI-based, and lexical overlap—experience significant performance degradation as retrieval quality decreases. These findings underscore the critical need for retrieval-aware hallucination detection and provide actionable insights for deploying trustworthy RAG systems in practice.

As RAG becomes increasingly prevalent in real-world applications, ensuring robustness to retrieval failures is essential. Our work highlights this overlooked challenge and provides a foundation for developing more resilient detection methods. We release our code and experimental framework to facilitate future research in this important area.

\section*{Acknowledgments}

[To be added]

\bibliographystyle{acl_natbib}
\bibliography{references}

\end{document}
