{"question": "where did they film hot tub time machine", "answer": "don't know. The provided context does not mention where they filmed \"Hot Tub Time Machine.\"", "context": [], "ground_truth": ["Fernie Alpine Resort"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who has the right of way in international waters", "answer": "don't know. The provided context does not contain information about the right of way in international waters.", "context": [], "ground_truth": ["Neither vessel"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who does annie work for attack on titan", "answer": "don't know. The provided context does not mention any character named Annie or her work in the Attack on Titan anime or manga.", "context": [], "ground_truth": ["Marley"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": true, "nli_entailment_prob": 0.0, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when was the immigration reform and control act passed", "answer": "don't know, the provided context does not mention any information about an immigration reform and control act being passed.", "context": [], "ground_truth": ["November\u00a06, 1986"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": false, "lexical_overlap": 0.5, "ground_truth_hallucinated": true}
{"question": "when was puerto rico added to the usa", "answer": "don't know. The provided context does not contain information about when Puerto Rico was added to the United States.", "context": [], "ground_truth": ["1950"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who has been chosen for best supporting actress in 64 national filmfare award", "answer": "don't know. The provided context does not mention anything about the 64th National Filmfare Awards or their winners.", "context": [], "ground_truth": ["Zaira Wasim"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": true, "nli_entailment_prob": 0.0, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "which side of the white house is the front", "answer": "don't know. The provided context does not contain any information about the white house or its location.", "context": [], "ground_truth": ["North"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "names of the metropolitan municipalities in south africa", "answer": "don't know. The provided context does not mention any metropolitan municipalities in South Africa.", "context": [], "ground_truth": ["Mangaung Metropolitan Municipality", "Nelson Mandela Bay Metropolitan Municipality", "eThekwini Metropolitan Municipality", "City of Tshwane Metropolitan Municipality", "City of Johannesburg Metropolitan Municipality", "Buffalo City Metropolitan Municipality", "City of Ekurhuleni Metropolitan Municipality"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": true, "nli_entailment_prob": 0.0, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who's hosting the super bowl in 2019", "answer": "don't know. The context does not provide information about the host of Super Bowl XLIX in 2019.", "context": [], "ground_truth": ["Atlanta, Georgia"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "in which year vivo launch its first phone in india", "answer": "don't know. The provided context does not contain information about the launch of Vivo's first phone in India.", "context": [], "ground_truth": ["2014"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "where does it talk about mary magdalene in the bible", "answer": "don't know, the context does not contain any information about Mary Magdalene.", "context": [], "ground_truth": ["New Testament"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": false, "ragas_faithfulness": 1.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who carries the nuclear football for the president", "answer": "don't know. The provided context does not contain information about who carries the nuclear football for the president.", "context": [], "ground_truth": ["aide-de-camp"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what is the origin of the name cynthia", "answer": "don't know. The provided context does not contain information about the origin of the name Cynthia.", "context": [], "ground_truth": ["Greek"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who is the guy who voiced disney channel", "answer": "don't know. The provided context does not mention anyone who voiced Disney Channel characters.", "context": [], "ground_truth": ["\"Buzz\" Brainard", "Cameron"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what's the legal marriage age in new york", "answer": "e legal marriage age in New York is not specified in the provided context.", "context": [], "ground_truth": ["18"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when do the red hot chili peppers tour", "answer": "don't know, the context does not mention the Red Hot Chili Peppers.", "context": [], "ground_truth": ["March 2018"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who plays mavis in the movie hotel transylvania", "answer": "don't know. The provided context does not mention anything about the movie Hotel Transylvania or its characters.", "context": [], "ground_truth": ["Sadie Sandler", "Selena Gomez"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": true, "nli_entailment_prob": 0.0, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what is the channel number for cartoon network on spectrum", "answer": "don't know. The provided context does not contain information about the channel number for Cartoon Network on spectrum.", "context": [], "ground_truth": ["768 (HD)", "68 (SD)"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when are the fa cup semi finals played", "answer": "don't know. The provided context does not mention when the FA Cup semi finals are played.", "context": [], "ground_truth": ["the new Wembley Stadium"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when did the ipod touch 6 gen came out", "answer": "e sixth generation iPod Touch was announced by Apple in mid-2015 and officially released on the Apple store on July 15, 2015.", "context": [], "ground_truth": ["July\u00a015,\u00a02015"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "west ham players in the 1966 world cup", "answer": "don't know. The provided context does not contain any information about West Ham players in the 1966 FIFA World Cup.", "context": [], "ground_truth": ["Martin Peters", "Geoff Hurst", "Bobby Moore"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who sings the theme song for miami vice", "answer": "don't know.", "context": [], "ground_truth": ["Jan Hammer"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5248289108276367, "lexical_hallucinated": false, "lexical_overlap": 0.5, "ground_truth_hallucinated": true}
{"question": "who sings should have seen it in color", "answer": "don't know. The provided context does not contain information about the song \"Should Have Seen It in Color\".", "context": [], "ground_truth": ["Jamey Johnson"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who starred in the movie deep water horizon", "answer": "don't know. The provided context does not mention anything about the movie Deep Water Horizon.", "context": [], "ground_truth": ["Kurt Russell", "Dylan O'Brien", "John Malkovich", "Kate Hudson", "Gina Rodriguez", "Mark Wahlberg"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when did justin bieber perform at madison square garden", "answer": "don't know. The provided context does not contain information about Justin Bieber performing at Madison Square Garden.", "context": [], "ground_truth": ["August 31, 2010"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "protozoan classification used to be based on their means of", "answer": "don't know. The provided context does not contain information about protozoan classification.", "context": [], "ground_truth": ["locomotion"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who was the first one to migrate to madinah", "answer": "e provided context does not contain information about the first one to migrate to Madinah.", "context": [], "ground_truth": ["Islamic prophet Muhammad"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "ice sheets and tundra are typical of which koppen climate category", "answer": "don't know. The provided context does not contain information about ice sheets and tundra being typical of a specific Koppen climate category.", "context": [], "ground_truth": ["polar"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what is the size of the angles of an equilateral triangle", "answer": "don't know. The provided context does not contain information about the size of the angles of an equilateral triangle.", "context": [], "ground_truth": ["60\u00b0"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when does beaty and the beast take place", "answer": "e provided context does not contain information about the movie or story \"Beauty and the Beast.\"", "context": [], "ground_truth": ["Rococo-era"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who sings done for me with charlie puth", "answer": "don't know.", "context": [], "ground_truth": ["American singer Kehlani"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5248289108276367, "lexical_hallucinated": false, "lexical_overlap": 0.5, "ground_truth_hallucinated": true}
{"question": "what counties are in the 6th congressional district in ga", "answer": "don't know. The provided context does not mention any counties in Georgia related to the 6th congressional district.", "context": [], "ground_truth": ["Fulton", "DeKalb", "Cobb"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who was heading indian army during bangladesh libration war 1971", "answer": "don't know. The provided context does not contain information about who was heading the Indian Army during the Bangladesh Liberation War in 1971.", "context": [], "ground_truth": ["Sam Manekshaw"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who wrote the music for somewhere in time", "answer": "don't know. The context does not mention who wrote the music for \"Somewhere in Time.\"", "context": [], "ground_truth": ["John Barry"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who sings the marriage of figaro in shawshank redemption", "answer": "don't know. The context provided does not mention anything about \"The Marriage of Figaro\" being sung in Shawshank Redemption.", "context": [], "ground_truth": ["Edith Mathis", "Gundula Janowitz"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "who did chris long play football for last year", "answer": "don't have information about Chris Long playing football last year.", "context": [], "ground_truth": ["Eagles"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when did harry potter and the deathly hallows come out", "answer": "don't know. The provided context does not contain information about the release date of Harry Potter and the Deathly Hallows.", "context": [], "ground_truth": ["21 July 2007"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "how many episode of my hero academia season 2", "answer": "don't know. The provided context does not contain information about My Hero Academia season 2.", "context": [], "ground_truth": ["25"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "during the rebuttal section of a formal debate which team speaks first", "answer": "don't know. The provided context does not contain information about the rebuttal section of a formal debate or the order in which teams speak during that section.", "context": [], "ground_truth": ["the affirmative team"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when was the last amendment made to the us constitution", "answer": "don't know, the provided context does not mention the last amendment made to the US Constitution.", "context": [], "ground_truth": ["May 5, 1992"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "starch based food to be made into plastic", "answer": "don't know. The provided context does not mention anything about starch based foods being made into plastic.", "context": [], "ground_truth": ["corn starch"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when did north korea launch the first missile", "answer": "don't know. The provided context does not mention anything about North Korea launching the first missile.", "context": [], "ground_truth": ["1984"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what mineral in 4 ga sandstones was dated to determine the age of the earth", "answer": "aphite in 3.7 billion-year-old metasedimentary rocks.", "context": [], "ground_truth": ["zircon"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": false, "lexical_overlap": 0.5, "ground_truth_hallucinated": true}
{"question": "when is the last time a total eclipse happened in the us", "answer": "don't know. The provided context does not contain information about the last total solar eclipse to occur in the United States.", "context": [], "ground_truth": ["August 21, 2017"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when did india participate in olympics for first time", "answer": "dia participated in the Olympics for the first time in 1900 at the Paris Games.", "context": [], "ground_truth": ["1900"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "when is it revealed that luke and leia are siblings", "answer": "e provided context does not contain information about Luke and Leia being siblings.", "context": [], "ground_truth": ["In Return of the Jedi"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what is the population of farmington new mexico", "answer": "don't know. The provided context does not contain information about the population of Farmington, New Mexico.", "context": [], "ground_truth": ["41,629"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": false, "ragas_faithfulness": 0.5, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "what was the last year they made the gmc envoy", "answer": "don't know. The provided context does not mention the last year that the GMC Envoy was made.", "context": [], "ground_truth": ["2008"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "new amsterdam was the main settlement in the dutch colony of", "answer": "w Amsterdam was the main settlement in the Dutch colony of New Netherland.", "context": [], "ground_truth": ["New Netherland"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
{"question": "in the bible who was thrown into the fiery furnace", "answer": "ere is no mention in the provided documents about anyone being thrown into a fiery furnace in the Bible.", "context": [], "ground_truth": ["Meshach", "Abednego", "Shadrach"], "quality_tier": "high", "num_relevant": 4, "num_distractors": 1, "ragas_hallucinated": true, "ragas_faithfulness": 0.0, "nli_hallucinated": false, "nli_entailment_prob": 0.5, "lexical_hallucinated": true, "lexical_overlap": 0.0, "ground_truth_hallucinated": true}
